import json
import os
import glob

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = "."
OUTPUT_FILE = "final_training_dataset.json"

# Ground Truth Mapping
# Maps source code function names to specific algorithm labels
LABEL_MAP = {
    # --- AES Variants ---
    "AES128_Encrypt": "AES-128",
    "AES192_Encrypt": "AES-192",
    "AES256_Encrypt": "AES-256",
    "AES_Encrypt": "AES-128", # Default from the simple aes.c
    "KeyExpansion": "AES-KeySchedule",
    
    # --- RSA Variants ---
    "RSA1024_Encrypt": "RSA-1024",
    "RSA2048_Encrypt": "RSA-2048",
    "RSA4096_Encrypt": "RSA-4096",
    "ModExp": "RSA-ModExp",     # The core math function for RSA
    
    # --- SHA Variants ---
    "sha1_transform": "SHA-1",
    "sha1_process": "SHA-1",
    "sha256_transform": "SHA-256",
    "sha256_process": "SHA-256",
    
    # --- Others ---
    "chacha20_block": "ChaCha20",
    "xor_encrypt": "XOR",
    "prng_next": "PRNG-LCG",
    
    # --- ECC Variants ---
    "ec_point_double": "ECC-SecP256",
    "ec_point_add": "ECC-SecP256",
    "ec_scalar_mult": "ECC-SecP256",
    "ecdh_compute_shared_secret": "ECC-SecP256",
    "ecdsa_sign": "ECC-SecP256",
    "ecdsa_verify": "ECC-SecP256"
}

def process_dataset():
    dataset = []
    
    # Find all JSON features files generated by Ghidra
    json_files = glob.glob(os.path.join(DATA_DIR, "*_features.json"))
    
    if not json_files:
        # Fallback to look for just .json if naming convention varies
        json_files = glob.glob(os.path.join(DATA_DIR, "*.json"))
        
    # Create output directories
    datasets_dir = os.path.join(SCRIPT_DIR, "datasets")
    by_arch_dir = os.path.join(datasets_dir, "by_arch")
    by_algo_dir = os.path.join(datasets_dir, "by_algo")
    
    for d in [datasets_dir, by_arch_dir, by_algo_dir]:
        if not os.path.exists(d):
            os.makedirs(d)

    # Containers for split datasets
    datasets_by_arch = {}
    datasets_by_algo = {}

    print(f"ðŸ·ï¸ Labeling {len(json_files)} binary analysis files...")

    for jf in json_files:
        try:
            with open(jf, 'r') as f:
                data = json.load(f)
        except Exception as e:
            print(f"âš ï¸ Error reading {jf}: {e}")
            continue
            
        binary_name = data.get('binary', '')
        
        # Extract Metadata
        parts = binary_name.split('_')
        meta = {
            "filename": binary_name,
            "arch": "unknown",
            "compiler": "unknown",
            "opt": "unknown",
            "inferred_algo_from_file": "unknown"
        }
        
        if len(parts) >= 4:
            meta["arch"] = parts[-3]
            meta["compiler"] = parts[-2]
            meta["opt"] = parts[-1].split('.')[0]
            meta["inferred_algo_from_file"] = "_".join(parts[:-3])

        for func in data['functions']:
            func_name = func['name']
            label = "Non-Crypto" 
            
            # 1. GOLD STANDARD
            matched = False
            func_lower = func_name.lower()
            for key, val in LABEL_MAP.items():
                if key.lower() in func_lower:
                    label = val
                    matched = True
                    break
            
            # 2. SILVER STANDARD
            if not matched:
                if "Cipher" in func_name:
                    if "aes256" in binary_name: label = "AES-256"
                    elif "aes192" in binary_name: label = "AES-192"
                    elif "aes128" in binary_name: label = "AES-128"
            
            # Map Ghidra output keys
            nodes_list = func.get('nodes', func.get('node_level', []))
            edges_list = func.get('edges', func.get('edge_level', []))
            graph_feats = func.get('graph_features', func.get('graph_level', {}))

            sample = {
                "id": f"{binary_name}::{func_name}",
                "label": label,
                "metadata": meta,
                "graph_features": graph_feats,
                "nodes": [], 
                "edges": edges_list 
            }
            
            for n in nodes_list:
                feats = n.get('features', n)
                vector = [
                    float(feats.get('instruction_count', 0)),
                    float(feats.get('bitwise_op_density', 0.0)),
                    float(feats.get('immediate_entropy', 0.0)),
                    1.0 if feats.get('table_lookup_presence', False) else 0.0,
                    float(feats.get('crypto_constant_hits', 0)),
                    float(feats.get('n_gram_repetition', 0.0)),
                    float(feats.get('carry_chain_depth', 0)),
                    1.0 if feats.get('simd_usage', False) else 0.0,
                    0.0 
                ]
                sample['nodes'].append(vector)

            # Add to Full Dataset
            dataset.append(sample)
            
            # Add to Architecture Split
            arch = meta["arch"]
            if arch not in datasets_by_arch: datasets_by_arch[arch] = []
            datasets_by_arch[arch].append(sample)
            
            # Add to Algorithm Split (only if labeled)
            if label != "Non-Crypto":
                if label not in datasets_by_algo: datasets_by_algo[label] = []
                datasets_by_algo[label].append(sample)

    print(f"âœ… Generated {len(dataset)} labeled function samples (Full).")
    
    # Save Full Dataset
    with open(OUTPUT_FILE, 'w') as f:
        json.dump(dataset, f, indent=2)
    print(f"ðŸ“‚ Saved full dataset to {OUTPUT_FILE}")
    
    # Save Architecture Splits
    for arch, samples in datasets_by_arch.items():
        fname = os.path.join(by_arch_dir, f"dataset_{arch}.json")
        with open(fname, 'w') as f:
            json.dump(samples, f, indent=2)
        print(f"   - Saved {len(samples)} samples to {fname}")
        
    # Save Algorithm Splits
    for algo, samples in datasets_by_algo.items():
        fname = os.path.join(by_algo_dir, f"dataset_{algo}.json")
        with open(fname, 'w') as f:
            json.dump(samples, f, indent=2)
        print(f"   - Saved {len(samples)} samples to {fname}")

if __name__ == "__main__":
    process_dataset()